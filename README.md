# Inverse RL + Policy Optimization for E-commerce Promotions
Learning from History to Optimize Offers and Drive Profit: IRL, Listwise Teacher Distillation, and Critic-Free Group Relative Policy Optimization
This paper presents a system that tackles a common, high-impact retail problem: deciding both when to run a promotion and which specific offer to show each customer.
The business objective is to maximize profit and encourage future repurchase, while avoiding customer fatigue and staying within practical constraints such as budget limits and inventory availability. Unlike traditional approaches such as response modeling or recommender systems, the framework here combines interpretability with policy optimization, adding measurable business value through better control, safer deployment, and more adaptive decision-making.
The process is trained offline using historical transaction and interaction data, making it risk-free for testing and straightforward to integrate into production systems. The first step applies inverse reinforcement learning (IRL) to learn from "implicit experts" hidden in the logs. Stage A models the decision boundary between promoting and holding, while Stage B captures preferences among offer types - coupon, free shipping, bundle, points, or message - producing an interpretable teacher model that explains historical actions before any reinforcement learning takes place.
Three learning approaches are then compared. The Listwise Teacher-Distilled Policy (LTDP) transforms the teacher's ranked preferences into a neural scoring model via cross-entropy, providing a strong supervised baseline. DeepSeek-GRPO, a critic-free, group-relative policy-gradient algorithm, evaluates all feasible offers within each session and updates the policy by comparing relative outcomes. An enhanced DeepSeek-GRPO further incorporates reference-centered advantages, temperature scheduling, and adaptive penalty multipliers to improve stability and control spend or policy drift.
Policy performance is evaluated using off-policy evaluation (OPE) techniques. Inverse Propensity Scoring (IPS) estimates how a new policy would perform on historical data by reweighting observed outcomes, while Self-Normalized IPS (SNIPS) reduces variance for more stable estimates. Across validation periods, the enhanced RL policies consistently outperform baseline methods, giving both higher returns and more reliable confidence intervals. This creates a closed business-ready loop: learn from historical patterns, optimize under constraints, and deploy daily recommendations with transparent reasoning that marketers can trust.
